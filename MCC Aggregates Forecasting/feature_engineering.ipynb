{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x45KJ3_nETBx"
   },
   "source": [
    "# Final Feature Engineering \n",
    "Ensures all models predict the same test periods for fair comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1558,
     "status": "ok",
     "timestamp": 1748874913489,
     "user": {
      "displayName": "V L",
      "userId": "06129047709976680873"
     },
     "user_tz": -180
    },
    "id": "HGLo9_2FETBz",
    "outputId": "e01fc02f-5304-4190-cc63-c35f4bacfcc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Engineering - Unified Test Dates for All Models\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Final Feature Engineering - Unified Test Dates for All Models\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17369,
     "status": "ok",
     "timestamp": 1748874930860,
     "user": {
      "displayName": "V L",
      "userId": "06129047709976680873"
     },
     "user_tz": -180
    },
    "id": "X6a26uL6ETB0",
    "outputId": "ade50cd5-4be7-47bb-bae6-20d3146688ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n",
      "Environment: colab\n",
      "Base path: /content/drive/MyDrive/fcst\n"
     ]
    }
   ],
   "source": [
    "def detect_environment():\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive/')\n",
    "        return 'colab', '/content/drive/MyDrive/fcst'\n",
    "    except ImportError:\n",
    "        return 'local', '..'\n",
    "\n",
    "environment, base_path = detect_environment()\n",
    "print(f\"Environment: {environment}\")\n",
    "print(f\"Base path: {base_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9625,
     "status": "ok",
     "timestamp": 1748874940631,
     "user": {
      "displayName": "V L",
      "userId": "06129047709976680873"
     },
     "user_tz": -180
    },
    "id": "tGYACM6_ETB0",
    "outputId": "92c1814b-cce1-40f5-9c10-e7f27f9e277d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and combining preprocessed data...\n",
      "‚úì Loading from parquet format...\n",
      "‚úì Successfully loaded train: 1,836,701 records\n",
      "‚úì Successfully loaded test: 462,255 records\n",
      "‚úì Combined dataset: 2,298,956 records\n",
      "‚úì Date range: 2009-12-28 to 2019-10-21\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading and combining preprocessed data...\")\n",
    "\n",
    "# Load the pre-split data with robust error handling\n",
    "train_parquet = f'{base_path}/data/preprocessed/train_with_target.parquet'\n",
    "test_parquet = f'{base_path}/data/preprocessed/test_with_target.parquet'\n",
    "train_csv = f'{base_path}/data/preprocessed/train_with_target.csv'\n",
    "test_csv = f'{base_path}/data/preprocessed/test_with_target.csv'\n",
    "\n",
    "# Try loading files with fallbacks\n",
    "try:\n",
    "    if os.path.exists(train_parquet) and os.path.exists(test_parquet):\n",
    "        print(\"‚úì Loading from parquet format...\")\n",
    "        train_df = pd.read_parquet(train_parquet)\n",
    "        test_df = pd.read_parquet(test_parquet)\n",
    "    elif os.path.exists(train_csv) and os.path.exists(test_csv):\n",
    "        print(\"‚úì Loading from CSV format...\")\n",
    "        train_df = pd.read_csv(train_csv)\n",
    "        test_df = pd.read_csv(test_csv)\n",
    "    else:\n",
    "        print(\"‚ùå Could not find preprocessed files\")\n",
    "        print(\"Please run preprocessing_fixed.py first to create the required files\")\n",
    "        print(f\"Looking for files in: {base_path}/data/preprocessed/\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"‚úì Successfully loaded train: {len(train_df):,} records\")\n",
    "    print(f\"‚úì Successfully loaded test: {len(test_df):,} records\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading preprocessed files: {e}\")\n",
    "    print(\"Please run preprocessing_fixed.py first\")\n",
    "    exit()\n",
    "\n",
    "# Combine for unified dataset\n",
    "try:\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "    combined_df = combined_df.sort_values(['client_id', 'category', 'date'])\n",
    "\n",
    "    print(f\"‚úì Combined dataset: {len(combined_df):,} records\")\n",
    "    print(f\"‚úì Date range: {combined_df['date'].min().date()} to {combined_df['date'].max().date()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error combining datasets: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56725,
     "status": "ok",
     "timestamp": 1748874997355,
     "user": {
      "displayName": "V L",
      "userId": "06129047709976680873"
     },
     "user_tz": -180
    },
    "id": "OcrrtcgjETB1",
    "outputId": "2f134b55-94f4-4bd6-fa55-9c5ad970d0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating split indicators...\n",
      "‚úì Split verification - train: 1,836,701, test: 462,255\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating split indicators...\")\n",
    "\n",
    "try:\n",
    "    # Mark records as train/test based on original split\n",
    "    train_dates = set(zip(train_df['client_id'], train_df['category'], pd.to_datetime(train_df['date'])))\n",
    "    test_dates = set(zip(test_df['client_id'], test_df['category'], pd.to_datetime(test_df['date'])))\n",
    "\n",
    "    combined_df['split'] = combined_df.apply(\n",
    "        lambda row: 'train' if (row['client_id'], row['category'], row['date']) in train_dates\n",
    "        else 'test', axis=1\n",
    "    )\n",
    "\n",
    "    train_count = (combined_df['split'] == 'train').sum()\n",
    "    test_count = (combined_df['split'] == 'test').sum()\n",
    "\n",
    "    print(f\"‚úì Split verification - train: {train_count:,}, test: {test_count:,}\")\n",
    "\n",
    "    if train_count == 0 or test_count == 0:\n",
    "        print(\"‚ùå Warning: One of the splits is empty!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating split indicators: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77532,
     "status": "ok",
     "timestamp": 1748875074885,
     "user": {
      "displayName": "V L",
      "userId": "06129047709976680873"
     },
     "user_tz": -180
    },
    "id": "KNOsHPT7ETB1",
    "outputId": "387090ab-16b8-4d36-dcbe-5370057dfc55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BASELINE & STATISTICAL MODELS ===\n",
      "‚úì Full dataset for baseline/statistical: 2,298,956 records\n",
      "‚úì Use 'split' column to identify train/test periods\n",
      "‚úì Saved: /content/drive/MyDrive/fcst/data/features/baseline_statistical_full.csv\n",
      "‚úì Saved: /content/drive/MyDrive/fcst/data/features/baseline_statistical_full.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== BASELINE & STATISTICAL MODELS ===\")\n",
    "\n",
    "try:\n",
    "    # Create output directory\n",
    "    os.makedirs(f'{base_path}/data/features', exist_ok=True)\n",
    "\n",
    "    # Save full dataset for baseline/statistical models\n",
    "    baseline_data = combined_df.copy()\n",
    "\n",
    "    # Save both formats\n",
    "    baseline_csv_path = f'{base_path}/data/features/baseline_statistical_full.csv'\n",
    "    baseline_parquet_path = f'{base_path}/data/features/baseline_statistical_full.parquet'\n",
    "\n",
    "    baseline_data.to_csv(baseline_csv_path, index=False)\n",
    "    baseline_data.to_parquet(baseline_parquet_path, index=False)\n",
    "\n",
    "    print(f\"‚úì Full dataset for baseline/statistical: {len(baseline_data):,} records\")\n",
    "    print(\"‚úì Use 'split' column to identify train/test periods\")\n",
    "    print(f\"‚úì Saved: {baseline_csv_path}\")\n",
    "    print(f\"‚úì Saved: {baseline_parquet_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving baseline data: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 457661,
     "status": "ok",
     "timestamp": 1748875532524,
     "user": {
      "displayName": "V L",
      "userId": "06129047709976680873"
     },
     "user_tz": -180
    },
    "id": "KuB9IYtzETB1",
    "outputId": "a908a63c-b1d3-4a65-fead-1a76a5c9bf68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ML FEATURES ===\n",
      "Creating ML features...\n",
      "‚úì Adding lag and rolling features...\n",
      "‚úì Calculating normalization statistics...\n",
      "‚úì Applying feature normalization...\n",
      "‚úì ML features created successfully\n",
      "‚úì Features shape: (2298956, 72)\n",
      "‚úì Train set: 1,836,701 records\n",
      "‚úì Test set: 462,255 records\n",
      "Saving ML features...\n",
      "‚úì Saved: /content/drive/MyDrive/fcst/data/features/ml_train.csv\n",
      "‚úì Saved: /content/drive/MyDrive/fcst/data/features/ml_train.parquet\n",
      "‚úì Saved: /content/drive/MyDrive/fcst/data/features/ml_test.csv\n",
      "‚úì Saved: /content/drive/MyDrive/fcst/data/features/ml_test.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== ML FEATURES ===\")\n",
    "\n",
    "def create_ml_features(df):\n",
    "    \"\"\"Create comprehensive features for ML models\"\"\"\n",
    "    try:\n",
    "        df = df.copy()\n",
    "\n",
    "        # Calendar features\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['quarter'] = df['date'].dt.quarter\n",
    "        df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "        df['day_of_week'] = df['date'].dt.dayofweek\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype('int8')\n",
    "\n",
    "        # Cyclical encoding\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12).astype('float32')\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12).astype('float32')\n",
    "        df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52).astype('float32')\n",
    "        df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52).astype('float32')\n",
    "\n",
    "        # Lag and rolling features\n",
    "        def add_ml_features(group):\n",
    "            group = group.sort_values('date')\n",
    "\n",
    "            # Lag features (past data only)\n",
    "            for lag in [1, 2, 4, 8, 12, 26, 52]:\n",
    "                group[f'amount_lag_{lag}'] = group['amount'].shift(lag)\n",
    "                group[f'log_amount_lag_{lag}'] = group['log_amount'].shift(lag)\n",
    "\n",
    "            # Rolling statistics (past data only)\n",
    "            for window in [4, 8, 12, 26, 52]:\n",
    "                group[f'amount_ma_{window}'] = group['amount'].rolling(window, min_periods=1).mean()\n",
    "                group[f'amount_std_{window}'] = group['amount'].rolling(window, min_periods=1).std()\n",
    "                group[f'amount_min_{window}'] = group['amount'].rolling(window, min_periods=1).min()\n",
    "                group[f'amount_max_{window}'] = group['amount'].rolling(window, min_periods=1).max()\n",
    "\n",
    "            return group\n",
    "\n",
    "        print(\"‚úì Adding lag and rolling features...\")\n",
    "        df = df.groupby(['client_id', 'category']).apply(add_ml_features).reset_index(drop=True)\n",
    "\n",
    "        # User features normalization (using training data stats)\n",
    "        user_features = ['current_age', 'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards']\n",
    "        train_mask = df['split'] == 'train'\n",
    "        train_stats = {}\n",
    "\n",
    "        print(\"‚úì Calculating normalization statistics...\")\n",
    "        for feature in user_features:\n",
    "            if feature in df.columns:\n",
    "                train_stats[f'{feature}_mean'] = df.loc[train_mask, feature].mean()\n",
    "                train_stats[f'{feature}_std'] = df.loc[train_mask, feature].std()\n",
    "\n",
    "        # Apply normalization\n",
    "        print(\"‚úì Applying feature normalization...\")\n",
    "        for feature in user_features:\n",
    "            if feature in df.columns:\n",
    "                mean = train_stats[f'{feature}_mean']\n",
    "                std = train_stats[f'{feature}_std']\n",
    "                df[f'{feature}_norm'] = (df[feature] - mean) / (std + 1e-8)\n",
    "\n",
    "        return df, train_stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in create_ml_features: {e}\")\n",
    "        raise\n",
    "\n",
    "# Create ML features\n",
    "print(\"Creating ML features...\")\n",
    "try:\n",
    "    ml_full, ml_train_stats = create_ml_features(combined_df)\n",
    "\n",
    "    # Split back into train/test\n",
    "    ml_train = ml_full[ml_full['split'] == 'train'].copy()\n",
    "    ml_test = ml_full[ml_full['split'] == 'test'].copy()\n",
    "\n",
    "    print(f\"‚úì ML features created successfully\")\n",
    "    print(f\"‚úì Features shape: {ml_full.shape}\")\n",
    "    print(f\"‚úì Train set: {len(ml_train):,} records\")\n",
    "    print(f\"‚úì Test set: {len(ml_test):,} records\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating ML features: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Save ML features\n",
    "try:\n",
    "    print(\"Saving ML features...\")\n",
    "\n",
    "    # Save CSV files\n",
    "    ml_train_csv = f'{base_path}/data/features/ml_train.csv'\n",
    "    ml_test_csv = f'{base_path}/data/features/ml_test.csv'\n",
    "    ml_train.to_csv(ml_train_csv, index=False)\n",
    "    ml_test.to_csv(ml_test_csv, index=False)\n",
    "\n",
    "    # Save Parquet files\n",
    "    ml_train_parquet = f'{base_path}/data/features/ml_train.parquet'\n",
    "    ml_test_parquet = f'{base_path}/data/features/ml_test.parquet'\n",
    "    ml_train.to_parquet(ml_train_parquet, index=False)\n",
    "    ml_test.to_parquet(ml_test_parquet, index=False)\n",
    "\n",
    "    print(f\"‚úì Saved: {ml_train_csv}\")\n",
    "    print(f\"‚úì Saved: {ml_train_parquet}\")\n",
    "    print(f\"‚úì Saved: {ml_test_csv}\")\n",
    "    print(f\"‚úì Saved: {ml_test_parquet}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving ML features: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1748875533235,
     "user": {
      "displayName": "V L",
      "userId": "06129047709976680873"
     },
     "user_tz": -180
    },
    "id": "Er1XVcanETB2",
    "outputId": "e816cb7d-2837-4aaa-f718-4bc3ce88e657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving normalization statistics...\n",
      "‚úì Saved: /content/drive/MyDrive/fcst/data/features/normalization_stats.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Saving normalization statistics...\")\n",
    "\n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    stats_for_json = {}\n",
    "    for k, v in ml_train_stats.items():\n",
    "        if pd.isna(v):\n",
    "            stats_for_json[k] = None\n",
    "        else:\n",
    "            stats_for_json[k] = float(v)\n",
    "\n",
    "    # Add split information\n",
    "    split_info = {\n",
    "        'environment': environment,\n",
    "        'train_records': len(ml_train),\n",
    "        'test_records': len(ml_test),\n",
    "        'total_records': len(combined_df),\n",
    "        'test_date_range': [ml_test['date'].min().isoformat(), ml_test['date'].max().isoformat()],\n",
    "        'feature_count': ml_full.shape[1],\n",
    "        'processing_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    stats_for_json['split_info'] = split_info\n",
    "\n",
    "    stats_file = f'{base_path}/data/features/normalization_stats.json'\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats_for_json, f, indent=2)\n",
    "\n",
    "    print(f\"‚úì Saved: {stats_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving normalization statistics: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1748875533550,
     "user": {
      "displayName": "V L",
      "userId": "06129047709976680873"
     },
     "user_tz": -180
    },
    "id": "0ZFIymRuETB2",
    "outputId": "28cdb8ba-aa63-4e71-ed9d-5f50cd542a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL FEATURE ENGINEERING SUMMARY\n",
      "============================================================\n",
      "Environment: colab\n",
      "\n",
      "üéØ UNIFIED TEST DATES FOR ALL MODELS:\n",
      "   All models predict the same 462,255 test records\n",
      "   Test period: 2012-06-11 to 2019-10-21\n",
      "\n",
      "Files created:\n",
      "1. Baseline & Statistical models:\n",
      "   - baseline_statistical_full.parquet/.csv (full dataset + split column)\n",
      "2. ML models:\n",
      "   - ml_train.parquet/.csv, ml_test.parquet/.csv\n",
      "3. Reference:\n",
      "   - normalization_stats.json\n",
      "\n",
      "Dataset Statistics:\n",
      "‚úì Total records: 2,298,956\n",
      "‚úì Training records: 1,836,701\n",
      "‚úì Test records: 462,255\n",
      "‚úì Features created: 72\n",
      "\n",
      "‚úÖ FAIR MODEL COMPARISON ENABLED:\n",
      "‚úì All models predict the same test dates\n",
      "‚úì No data leakage - features use only past information\n",
      "‚úì Environment compatible (Colab/Local)\n",
      "‚úì Ready for comprehensive model evaluation!\n",
      "\n",
      "Feature engineering completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Environment: {environment}\")\n",
    "print(\"\\nüéØ UNIFIED TEST DATES FOR ALL MODELS:\")\n",
    "print(f\"   All models predict the same {len(ml_test):,} test records\")\n",
    "print(f\"   Test period: {ml_test['date'].min().date()} to {ml_test['date'].max().date()}\")\n",
    "\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"1. Baseline & Statistical models:\")\n",
    "print(\"   - baseline_statistical_full.parquet/.csv (full dataset + split column)\")\n",
    "print(\"2. ML models:\")\n",
    "print(\"   - ml_train.parquet/.csv, ml_test.parquet/.csv\")\n",
    "print(\"3. Reference:\")\n",
    "print(\"   - normalization_stats.json\")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"‚úì Total records: {len(combined_df):,}\")\n",
    "print(f\"‚úì Training records: {len(ml_train):,}\")\n",
    "print(f\"‚úì Test records: {len(ml_test):,}\")\n",
    "print(f\"‚úì Features created: {ml_full.shape[1]}\")\n",
    "\n",
    "print(\"\\n‚úÖ FAIR MODEL COMPARISON ENABLED:\")\n",
    "print(\"‚úì All models predict the same test dates\")\n",
    "print(\"‚úì No data leakage - features use only past information\")\n",
    "print(\"‚úì Environment compatible (Colab/Local)\")\n",
    "print(\"‚úì Ready for comprehensive model evaluation!\")\n",
    "print(\"\\nFeature engineering completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{"cells":[{"cell_type":"markdown","metadata":{"id":"GDFA8ed5YeQE"},"source":["# Simplified Deep Learning Features\n","Creates sequence data for LSTM, TFT and other DL models with simplified CPU processing\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSC8eVjJYeQH","executionInfo":{"status":"ok","timestamp":1748880591131,"user_tz":-180,"elapsed":500,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"dea1ec13-d22f-4bae-a1ed-fe0b25776e39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting simplified DL feature engineering...\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import os\n","from datetime import datetime\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"Starting simplified DL feature engineering...\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYoqn5TkYeQI","executionInfo":{"status":"ok","timestamp":1748880615064,"user_tz":-180,"elapsed":23932,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"8466b916-42d7-4aa2-db20-46aa0d953f9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","âœ“ cuDF available - GPU dataframe operations enabled\n","âœ“ GPU memory: 42.5 GB\n","Environment: colab\n","GPU support: cudf\n","cuDF available: True\n"]}],"source":["def detect_environment():\n","    try:\n","        import google.colab\n","        from google.colab import drive\n","        drive.mount('/content/drive/')\n","        return 'colab', '/content/drive/MyDrive/fcst'\n","    except ImportError:\n","        return 'local', '..'\n","\n","def install_cudf_colab():\n","    \"\"\"Install cuDF in Google Colab if not available\"\"\"\n","    try:\n","        import cudf\n","        print(\"âœ“ cuDF already available\")\n","        return True\n","    except ImportError:\n","        print(\"âš  cuDF not found. Installing in Colab...\")\n","        try:\n","            import google.colab\n","            import subprocess\n","            import sys\n","\n","            # Install cuDF for CUDA 11\n","            subprocess.check_call([\n","                sys.executable, \"-m\", \"pip\", \"install\",\n","                \"cudf-cu11\", \"--extra-index-url=https://pypi.nvidia.com\"\n","            ])\n","            subprocess.check_call([\n","                sys.executable, \"-m\", \"pip\", \"install\", \"cupy-cuda11x\"\n","            ])\n","\n","            print(\"âœ“ cuDF installed successfully\")\n","            print(\"âš  Please restart runtime and run again\")\n","            return False\n","        except:\n","            print(\"âš  Failed to install cuDF\")\n","            return False\n","\n","def setup_gpu():\n","    \"\"\"Setup GPU for deep learning feature engineering with cuDF support\"\"\"\n","    gpu_info = {'available': False, 'type': None, 'device': None, 'cudf_available': False}\n","\n","    # Check cuDF first (for GPU dataframes)\n","    try:\n","        import cudf\n","        import cupy as cp\n","        gpu_info['cudf_available'] = True\n","        gpu_info['available'] = True\n","        gpu_info['type'] = 'cudf'\n","        print(f\"âœ“ cuDF available - GPU dataframe operations enabled\")\n","        print(f\"âœ“ GPU memory: {cp.cuda.runtime.memGetInfo()[1] / 1e9:.1f} GB\")\n","    except ImportError:\n","        print(\"âš  cuDF not available, trying PyTorch...\")\n","\n","        # Try to install in Colab\n","        environment, _ = detect_environment()\n","        if environment == 'colab':\n","            if install_cudf_colab():\n","                return setup_gpu()  # Retry after installation\n","\n","    # Fallback to PyTorch\n","    if not gpu_info['available']:\n","        try:\n","            import torch\n","            if torch.cuda.is_available():\n","                gpu_info['available'] = True\n","                gpu_info['type'] = 'cuda'\n","                gpu_info['device'] = torch.device('cuda')\n","                print(f\"âœ“ CUDA GPU: {torch.cuda.get_device_name(0)}\")\n","            elif torch.backends.mps.is_available():\n","                gpu_info['available'] = True\n","                gpu_info['type'] = 'mps'\n","                gpu_info['device'] = torch.device('mps')\n","                print(\"âœ“ Apple MPS GPU available\")\n","            else:\n","                print(\"âš  GPU available but not CUDA/MPS compatible\")\n","                gpu_info['device'] = torch.device('cpu')\n","        except ImportError:\n","            print(\"âš  PyTorch not available, using CPU only\")\n","            gpu_info['device'] = None\n","\n","    return gpu_info\n","\n","environment, base_path = detect_environment()\n","gpu_info = setup_gpu()\n","\n","print(f\"Environment: {environment}\")\n","print(f\"GPU support: {gpu_info['type'] if gpu_info['available'] else 'CPU only'}\")\n","print(f\"cuDF available: {gpu_info['cudf_available']}\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyrjRhmHYeQJ","executionInfo":{"status":"ok","timestamp":1748880619985,"user_tz":-180,"elapsed":4924,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"53a0f322-77cb-4376-c45a-e51f46ed3b77"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading preprocessed data...\n","Loading unified dataset...\n","Dataset: 2,298,956 records\n","Train: 1,836,701, Test: 462,255\n"]}],"source":["print(\"\\nLoading preprocessed data...\")\n","\n","# Load the unified data from feature_engineering_final.py\n","baseline_file = f'{base_path}/data/features/baseline_statistical_full.parquet'\n","if os.path.exists(baseline_file):\n","    print(\"Loading unified dataset...\")\n","    df = pd.read_parquet(baseline_file)\n","else:\n","    # Fallback to separate files\n","    print(\"Loading from separate train/test files...\")\n","    train_df = pd.read_parquet(f'{base_path}/data/preprocessed/train_with_target.parquet')\n","    test_df = pd.read_parquet(f'{base_path}/data/preprocessed/test_with_target.parquet')\n","\n","    # Combine and mark splits\n","    df = pd.concat([train_df, test_df], ignore_index=True)\n","    train_indices = set(zip(train_df['client_id'], train_df['category'], pd.to_datetime(train_df['date'])))\n","    df['split'] = df.apply(\n","        lambda row: 'train' if (row['client_id'], row['category'], pd.to_datetime(row['date'])) in train_indices\n","        else 'test', axis=1\n","    )\n","\n","df['date'] = pd.to_datetime(df['date'])\n","df = df.sort_values(['client_id', 'category', 'date'])\n","\n","print(f\"Dataset: {len(df):,} records\")\n","print(f\"Train: {(df['split'] == 'train').sum():,}, Test: {(df['split'] == 'test').sum():,}\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvKdyQSGYeQJ","executionInfo":{"status":"ok","timestamp":1748881329875,"user_tz":-180,"elapsed":709887,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"3d0e20f4-35ac-427e-b17a-e17227b88566"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== CREATING SIMPLIFIED DL SEQUENCES ===\n","Using 20% sample for faster processing...\n","Sample size: 459,791 records (from 2,298,956 total)\n","Creating simplified sequences (lookback=52, min_length=60)...\n","âœ“ Created 159,659 sequences\n","DL sequences created: 159,659 records\n","Train sequences: 81,815\n","Test sequences: 77,844\n"]}],"source":["print(\"\\n=== CREATING SIMPLIFIED DL SEQUENCES ===\")\n","\n","# Use 20% sample for speed\n","print(\"Using 20% sample for faster processing...\")\n","df_sample = df.sample(frac=0.2, random_state=42).sort_values(['client_id', 'category', 'date'])\n","print(f\"Sample size: {len(df_sample):,} records (from {len(df):,} total)\")\n","\n","def create_simplified_sequences(df, lookback=52, min_series_length=60):\n","    \"\"\"Create simplified sequences without redundant features\"\"\"\n","    print(f\"Creating simplified sequences (lookback={lookback}, min_length={min_series_length})...\")\n","\n","    sequences = []\n","\n","    for (client_id, category), group in df.groupby(['client_id', 'category']):\n","        group = group.sort_values('date')\n","\n","        if len(group) < min_series_length:\n","            continue\n","\n","        for i in range(lookback, len(group)):\n","            sequence_data = group.iloc[i-lookback:i].copy()\n","            current_row = group.iloc[i].copy()\n","\n","            seq_record = {\n","                'client_id': client_id,\n","                'category': category,\n","                'date': current_row['date'],\n","                'split': current_row['split'],\n","\n","                # SINGLE TARGET\n","                'target': current_row['target'],\n","\n","                # PREDICTION TIME CONTEXT\n","                'pred_month': current_row['date'].month,\n","                'pred_quarter': current_row['date'].quarter,\n","\n","                # STATIC USER FEATURES\n","                'yearly_income': current_row.get('yearly_income', 0),\n","                'total_debt': current_row.get('total_debt', 0),\n","                'credit_score': current_row.get('credit_score', 0),\n","                'current_age': current_row.get('current_age', 0),\n","            }\n","\n","            # HISTORICAL SEQUENCE (amount + month only, no redundant features)\n","            for j in range(lookback):\n","                week_data = sequence_data.iloc[j]\n","                seq_record[f'hist_amount_{j}'] = week_data['amount']\n","                seq_record[f'hist_month_{j}'] = week_data['date'].month\n","\n","            # BASIC SEQUENCE STATISTICS (essential only)\n","            amounts = sequence_data['amount']\n","            seq_record['hist_mean'] = amounts.mean()\n","            seq_record['hist_std'] = amounts.std()\n","            seq_record['hist_trend'] = (amounts.iloc[-1] - amounts.iloc[0]) / lookback\n","\n","            sequences.append(seq_record)\n","\n","    print(f\"âœ“ Created {len(sequences):,} sequences\")\n","    return pd.DataFrame(sequences)\n","\n","# Create simplified sequences\n","dl_sequences = create_simplified_sequences(df_sample, lookback=52, min_series_length=60)\n","\n","print(f\"DL sequences created: {len(dl_sequences):,} records\")\n","print(f\"Train sequences: {(dl_sequences['split'] == 'train').sum():,}\")\n","print(f\"Test sequences: {(dl_sequences['split'] == 'test').sum():,}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"p7Wu-RgmYeQJ","executionInfo":{"status":"ok","timestamp":1748881329889,"user_tz":-180,"elapsed":6,"user":{"displayName":"V L","userId":"06129047709976680873"}}},"outputs":[],"source":["def gpu_normalize_batch(data_matrix, means, stds, gpu_info):\n","    \"\"\"Batch normalize multiple columns on GPU if available\"\"\"\n","    if gpu_info['cudf_available']:\n","        return _normalize_cudf(data_matrix, means, stds)\n","    elif gpu_info['available'] and len(data_matrix) > 10000:\n","        return _normalize_torch(data_matrix, means, stds, gpu_info)\n","    else:\n","        return (data_matrix - means) / (stds + 1e-8)\n","\n","def _normalize_cudf(data_matrix, means, stds):\n","    \"\"\"Normalization using cuDF/cupy\"\"\"\n","    try:\n","        import cudf\n","        import cupy as cp\n","\n","        # Convert to cupy arrays\n","        cp_data = cp.asarray(data_matrix)\n","        cp_means = cp.asarray(means)\n","        cp_stds = cp.asarray(stds)\n","\n","        # Normalize on GPU\n","        normalized = (cp_data - cp_means) / (cp_stds + 1e-8)\n","\n","        # Return to CPU\n","        result = cp.asnumpy(normalized)\n","        print(f\"  âœ“ cuDF GPU normalization: {data_matrix.shape} -> {result.shape}\")\n","        return result\n","\n","    except Exception as e:\n","        print(f\"  âš  cuDF normalization failed: {e}, using CPU\")\n","        return (data_matrix - means) / (stds + 1e-8)\n","\n","def _normalize_torch(data_matrix, means, stds, gpu_info):\n","    \"\"\"Normalization using PyTorch\"\"\"\n","    try:\n","        import torch\n","\n","        tensor_data = torch.tensor(data_matrix, device=gpu_info['device'], dtype=torch.float32)\n","        tensor_means = torch.tensor(means, device=gpu_info['device'], dtype=torch.float32)\n","        tensor_stds = torch.tensor(stds, device=gpu_info['device'], dtype=torch.float32)\n","\n","        normalized = (tensor_data - tensor_means) / (tensor_stds + 1e-8)\n","\n","        result = normalized.cpu().numpy()\n","        print(f\"  âœ“ PyTorch GPU normalization: {data_matrix.shape} -> {result.shape}\")\n","        return result\n","\n","    except Exception as e:\n","        print(f\"  âš  PyTorch normalization failed: {e}, using CPU\")\n","        return (data_matrix - means) / (stds + 1e-8)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IL4etR4TYeQK","executionInfo":{"status":"ok","timestamp":1748881331242,"user_tz":-180,"elapsed":1345,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"7987e6bf-bba9-4cf1-c81e-3045e5d236d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== GPU-ACCELERATED NORMALIZATION ===\n","Normalizing 111 numeric columns...\n","Features: 52 hist_amount, 52 hist_month, 4 static, 3 statistics\n","Data shape: (159659, 111)\n","GPU type: cudf\n","  âœ“ cuDF GPU normalization: (159659, 111) -> (159659, 111)\n","âœ“ Normalized 111 features using training statistics\n"]}],"source":["print(\"\\n=== GPU-ACCELERATED NORMALIZATION ===\")\n","\n","if len(dl_sequences) > 0:\n","    # Identify columns for normalization\n","    numeric_cols = []\n","\n","    # Historical sequence features (updated naming)\n","    for i in range(52):\n","        numeric_cols.extend([f'hist_amount_{i}', f'hist_month_{i}'])\n","\n","    # Static user features\n","    static_features = ['yearly_income', 'total_debt', 'credit_score', 'current_age']\n","    numeric_cols.extend(static_features)\n","\n","    # Historical sequence statistics\n","    hist_stats = ['hist_mean', 'hist_std', 'hist_trend']\n","    numeric_cols.extend(hist_stats)\n","\n","    # Filter existing columns\n","    numeric_cols = [col for col in numeric_cols if col in dl_sequences.columns]\n","\n","    print(f\"Normalizing {len(numeric_cols)} numeric columns...\")\n","    print(f\"Features: {len([c for c in numeric_cols if 'hist_amount' in c])} hist_amount, \"\n","          f\"{len([c for c in numeric_cols if 'hist_month' in c])} hist_month, \"\n","          f\"{len([c for c in numeric_cols if c in static_features])} static, \"\n","          f\"{len([c for c in numeric_cols if c in hist_stats])} statistics\")\n","\n","    # Compute normalization stats from training data only\n","    train_mask = dl_sequences['split'] == 'train'\n","    train_data = dl_sequences.loc[train_mask, numeric_cols]\n","\n","    train_means = train_data.mean().values\n","    train_stds = train_data.std().values\n","\n","    # Handle zero std (constant columns)\n","    train_stds = np.where(train_stds == 0, 1.0, train_stds)\n","\n","    # Apply GPU normalization to full dataset\n","    all_data = dl_sequences[numeric_cols].values\n","\n","    print(f\"Data shape: {all_data.shape}\")\n","    print(f\"GPU type: {gpu_info['type'] if gpu_info['available'] else 'CPU'}\")\n","\n","    normalized_data = gpu_normalize_batch(all_data, train_means, train_stds, gpu_info)\n","\n","    # Add normalized columns\n","    for i, col in enumerate(numeric_cols):\n","        dl_sequences[f'{col}_norm'] = normalized_data[:, i]\n","\n","    print(f\"âœ“ Normalized {len(numeric_cols)} features using training statistics\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRAC5HkcYeQK","executionInfo":{"status":"ok","timestamp":1748881331469,"user_tz":-180,"elapsed":224,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"3d877a20-1e5a-49a0-cd0d-163dada533f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== PREPARING TRAINING DATA ===\n","âœ“ Features (X): (159659, 113) - 113 columns\n","âœ“ Targets (y): (159659,)\n","âœ“ Train samples: 81,815\n","âœ“ Test samples: 77,844\n","\n","Train set: X_train(81815, 113), y_train(81815,)\n","Test set: X_test(77844, 113), y_test(77844,)\n"]}],"source":["print(\"\\n=== PREPARING TRAINING DATA ===\")\n","\n","def prepare_training_data(df):\n","    \"\"\"Separate features (X) from targets (y) for proper model training\"\"\"\n","\n","    # FEATURES (X) - everything except target\n","    feature_cols = []\n","\n","    # Historical sequence features\n","    for i in range(52):\n","        feature_cols.extend([f'hist_amount_{i}_norm', f'hist_month_{i}'])\n","\n","    # Static features\n","    feature_cols.extend(['yearly_income_norm', 'total_debt_norm', 'credit_score_norm', 'current_age_norm'])\n","\n","    # Sequence statistics\n","    feature_cols.extend(['hist_mean_norm', 'hist_std_norm', 'hist_trend_norm'])\n","\n","    # Prediction context\n","    feature_cols.extend(['pred_month', 'pred_quarter'])\n","\n","    # Filter existing columns\n","    feature_cols = [col for col in feature_cols if col in df.columns]\n","\n","    # TARGETS (y)\n","    targets = df['target'].values\n","\n","    # FEATURES (X)\n","    features = df[feature_cols].values\n","\n","    # METADATA\n","    metadata = df[['client_id', 'category', 'date', 'split']].copy()\n","\n","    print(f\"âœ“ Features (X): {features.shape} - {len(feature_cols)} columns\")\n","    print(f\"âœ“ Targets (y): {targets.shape}\")\n","    print(f\"âœ“ Train samples: {(df['split'] == 'train').sum():,}\")\n","    print(f\"âœ“ Test samples: {(df['split'] == 'test').sum():,}\")\n","\n","    return features, targets, metadata, feature_cols\n","\n","# Prepare training data\n","X, y, metadata, feature_names = prepare_training_data(dl_sequences)\n","\n","# Split train/test\n","train_mask = metadata['split'] == 'train'\n","test_mask = metadata['split'] == 'test'\n","\n","X_train, y_train = X[train_mask], y[train_mask]\n","X_test, y_test = X[test_mask], y[test_mask]\n","\n","print(f\"\\nTrain set: X_train{X_train.shape}, y_train{y_train.shape}\")\n","print(f\"Test set: X_test{X_test.shape}, y_test{y_test.shape}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_wrFhmwYeQK","executionInfo":{"status":"ok","timestamp":1748881547378,"user_tz":-180,"elapsed":215828,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"e603d089-efaf-4d9e-c714-a5dccfa0d7fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== CREATING SPECIALIZED DL FORMATS ===\n","LSTM format: 159,659 sequences\n","TFT format: 8,461,927 records\n"]}],"source":["print(\"\\n=== CREATING SPECIALIZED DL FORMATS ===\")\n","\n","# 1. LSTM Format: (samples, timesteps, features)\n","def create_lstm_format(df):\n","    \"\"\"Create LSTM-ready sequences with simplified features\"\"\"\n","    lstm_data = []\n","\n","    for idx, row in df.iterrows():\n","        # Create sequence matrix (52 timesteps x 2 features)\n","        sequence = []\n","        for i in range(52):\n","            timestep = [\n","                row[f'hist_amount_{i}_norm'],\n","                row[f'hist_month_{i}'] / 12.0,  # Normalize month\n","            ]\n","            sequence.append(timestep)\n","\n","        lstm_data.append({\n","            'client_id': row['client_id'],\n","            'category': row['category'],\n","            'date': row['date'],\n","            'split': row['split'],\n","            'sequence': sequence,  # 52 x 2 array\n","            'target': row['target'],\n","            # Static features\n","            'yearly_income_norm': row.get('yearly_income_norm', 0),\n","            'total_debt_norm': row.get('total_debt_norm', 0),\n","            'credit_score_norm': row.get('credit_score_norm', 0),\n","            'current_age_norm': row.get('current_age_norm', 0),\n","            # Prediction context\n","            'pred_month': row['pred_month'],\n","            'pred_quarter': row['pred_quarter'],\n","            # Sequence stats\n","            'hist_mean_norm': row.get('hist_mean_norm', 0),\n","            'hist_std_norm': row.get('hist_std_norm', 0),\n","            'hist_trend_norm': row.get('hist_trend_norm', 0),\n","        })\n","\n","    return pd.DataFrame(lstm_data)\n","\n","# 2. TFT Format: flat features with time index\n","def create_tft_format(df):\n","    \"\"\"Create TFT-ready format with simplified features\"\"\"\n","    tft_data = []\n","\n","    for idx, row in df.iterrows():\n","        # Create one record per timestep in the sequence\n","        for i in range(52):\n","            tft_record = {\n","                'series_id': f\"{row['client_id']}_{row['category']}\",\n","                'time_idx': i,\n","                'split': row['split'],\n","\n","                # Time-varying features (historical)\n","                'amount': row[f'hist_amount_{i}_norm'],\n","                'month': row[f'hist_month_{i}'],\n","\n","                # Static features (same for all timesteps)\n","                'yearly_income': row.get('yearly_income_norm', 0),\n","                'total_debt': row.get('total_debt_norm', 0),\n","                'credit_score': row.get('credit_score_norm', 0),\n","                'current_age': row.get('current_age_norm', 0),\n","            }\n","            tft_data.append(tft_record)\n","\n","        # Add target record (what we're predicting)\n","        target_record = {\n","            'series_id': f\"{row['client_id']}_{row['category']}\",\n","            'time_idx': 52,  # Next timestep\n","            'split': row['split'],\n","            'target': row['target'],\n","\n","            # Use prediction context\n","            'month': row['pred_month'],\n","\n","            # Static features\n","            'yearly_income': row.get('yearly_income_norm', 0),\n","            'total_debt': row.get('total_debt_norm', 0),\n","            'credit_score': row.get('credit_score_norm', 0),\n","            'current_age': row.get('current_age_norm', 0),\n","        }\n","        tft_data.append(target_record)\n","\n","    return pd.DataFrame(tft_data)\n","\n","# Create specialized formats\n","lstm_format = create_lstm_format(dl_sequences)\n","tft_format = create_tft_format(dl_sequences)\n","\n","print(f\"LSTM format: {len(lstm_format):,} sequences\")\n","print(f\"TFT format: {len(tft_format):,} records\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3IEOomYYeQL","executionInfo":{"status":"ok","timestamp":1748881550823,"user_tz":-180,"elapsed":3435,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"ae1f5414-4887-4f4b-d9d2-c1d5c4d7be17"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","LSTM arrays:\n","Train - Sequences: (81815, 52, 2), Targets: (81815,), Static: (81815, 9)\n","Test - Sequences: (77844, 52, 2), Targets: (77844,), Static: (77844, 9)\n"]}],"source":["def extract_lstm_arrays(lstm_df, split='train'):\n","    \"\"\"Extract LSTM sequences as numpy arrays for training\"\"\"\n","    subset = lstm_df[lstm_df['split'] == split]\n","\n","    # Sequences: (samples, timesteps, features)\n","    sequences = np.array([seq for seq in subset['sequence'].values])\n","\n","    # Targets\n","    targets = subset['target'].values\n","\n","    # Static features (can be concatenated or used separately)\n","    static_features = subset[['yearly_income_norm', 'total_debt_norm', 'credit_score_norm',\n","                             'current_age_norm', 'pred_month', 'pred_quarter',\n","                             'hist_mean_norm', 'hist_std_norm', 'hist_trend_norm']].values\n","\n","    print(f\"{split.title()} - Sequences: {sequences.shape}, Targets: {targets.shape}, Static: {static_features.shape}\")\n","    return sequences, targets, static_features\n","\n","# Example usage for LSTM\n","print(\"\\nLSTM arrays:\")\n","lstm_train_seq, lstm_train_y, lstm_train_static = extract_lstm_arrays(lstm_format, 'train')\n","lstm_test_seq, lstm_test_y, lstm_test_static = extract_lstm_arrays(lstm_format, 'test')\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZlfY3KmPYeQL","executionInfo":{"status":"ok","timestamp":1748881653468,"user_tz":-180,"elapsed":102625,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"f6598e80-6224-4f88-b430-2b4cfca2a666"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== SAVING DL FEATURES ===\n"]}],"source":["print(\"\\n=== SAVING DL FEATURES ===\")\n","\n","output_dir = f'{base_path}/data/features'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# 1. Full sequences with all features\n","dl_sequences.to_csv(f'{output_dir}/dl_sequences_full.csv', index=False)\n","dl_sequences.to_parquet(f'{output_dir}/dl_sequences_full.parquet', index=False)\n","\n","# 2. Train/test splits for convenience\n","dl_train = dl_sequences[dl_sequences['split'] == 'train'].copy()\n","dl_test = dl_sequences[dl_sequences['split'] == 'test'].copy()\n","\n","dl_train.to_csv(f'{output_dir}/dl_sequences_train.csv', index=False)\n","dl_test.to_csv(f'{output_dir}/dl_sequences_test.csv', index=False)\n","dl_train.to_parquet(f'{output_dir}/dl_sequences_train.parquet', index=False)\n","dl_test.to_parquet(f'{output_dir}/dl_sequences_test.parquet', index=False)\n","\n","# 3. LSTM format\n","lstm_format.to_parquet(f'{output_dir}/dl_lstm_format.parquet', index=False)\n","\n","# 4. TFT format\n","tft_format.to_parquet(f'{output_dir}/dl_tft_format.parquet', index=False)\n","\n","# 5. Save normalization stats for inference\n","normalization_stats = {}\n","if len(numeric_cols) > 0:\n","    for i, col in enumerate(numeric_cols):\n","        normalization_stats[f'{col}_mean'] = float(train_means[i])\n","        normalization_stats[f'{col}_std'] = float(train_stds[i])\n","\n","normalization_stats['dl_info'] = {\n","    'lookback_length': 52,\n","    'sequence_features': 2,  # hist_amount, hist_month\n","    'static_features': 4,    # yearly_income, total_debt, credit_score, current_age\n","    'prediction_features': 2, # pred_month, pred_quarter\n","    'sequence_statistics': 3, # hist_mean, hist_std, hist_trend\n","    'total_sequences': len(dl_sequences),\n","    'train_sequences': len(dl_train),\n","    'test_sequences': len(dl_test),\n","    'data_leakage_fixed': True,\n","    'targets_separated': True,\n","}\n","\n","with open(f'{output_dir}/dl_normalization_stats.json', 'w') as f:\n","    json.dump(normalization_stats, f, indent=2)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7_mbARKYeQL","executionInfo":{"status":"ok","timestamp":1748881653573,"user_tz":-180,"elapsed":41,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"54e49346-e8c5-4522-f848-2e94c1ce6cfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== SIMPLIFIED DL FEATURE ENGINEERING SUMMARY ===\n","Environment: colab\n","Processing: CPU-based with 20% sample\n","Lookback window: 52 weeks\n","Sequence features: 2 per timestep (amount, month)\n","\n","Dataset statistics:\n","  Sample size: 459,791 records (20% of total)\n","  Total sequences: 159,659\n","  Train sequences: 81,815\n","  Test sequences: 77,844\n","  Features per sequence: 111 normalized\n","\n","Files created:\n","1. Full sequences:\n","   - dl_sequences_full.parquet (all features)\n","2. Train/test splits:\n","   - dl_sequences_train.parquet\n","   - dl_sequences_test.parquet\n","3. Model-specific formats:\n","   - dl_lstm_format.parquet (for LSTM models)\n","   - dl_tft_format.parquet (for TFT models)\n","4. Reference:\n","   - dl_normalization_stats.json\n","\n","ðŸŽ¯ UNIFIED TEST DATES:\n","âœ“ All DL models predict the same 77,844 test sequences\n","âœ“ Same test period as baseline/ML models\n","\n","ðŸ”’ DATA LEAKAGE PREVENTION:\n","âœ“ Single target separated from input features\n","âœ“ No 'current period' features in inputs\n","âœ“ Only historical data (hist_) used for prediction\n","âœ“ Static features constant over time\n","âœ“ Prediction time features (pred_) for context only\n","\n","ðŸ“Š SIMPLIFIED FEATURE STRUCTURE:\n","âœ“ Historical sequence: 104 features (52 weeks Ã— 2 per week)\n","âœ“ Static user features: 4 features\n","âœ“ Sequence statistics: 3 features\n","âœ“ Prediction time context: 2 features\n","âœ“ Total input features: 113\n","\n","âš¡ PERFORMANCE:\n","âœ“ 20% sample for faster processing\n","âœ“ Removed redundant features\n","âœ“ Parquet format for fast I/O\n","âœ“ Proper temporal constraints (no data leakage)\n","\n","ðŸš€ Ready for model training!\n","   - LSTM: Use extract_lstm_arrays() function\n","   - TFT: Use dl_tft_format.parquet\n","   - Custom: Use X_train, y_train arrays\n","\n","ðŸ’¡ TRAINING EXAMPLES:\n","   # LSTM: lstm_train_seq(81815, 52, 2), lstm_train_y(81815,)\n","   # Dense: X_train(81815, 113), y_train(81815,)\n","   # Target properly separated for supervised learning\n"]}],"source":["print(\"\\n=== SIMPLIFIED DL FEATURE ENGINEERING SUMMARY ===\")\n","print(f\"Environment: {environment}\")\n","print(f\"Processing: CPU-based with 20% sample\")\n","print(f\"Lookback window: 52 weeks\")\n","print(f\"Sequence features: 2 per timestep (amount, month)\")\n","\n","print(f\"\\nDataset statistics:\")\n","print(f\"  Sample size: {len(df_sample):,} records (20% of total)\")\n","print(f\"  Total sequences: {len(dl_sequences):,}\")\n","print(f\"  Train sequences: {len(dl_train):,}\")\n","print(f\"  Test sequences: {len(dl_test):,}\")\n","print(f\"  Features per sequence: {len([c for c in dl_sequences.columns if c.endswith('_norm')])} normalized\")\n","\n","print(f\"\\nFiles created:\")\n","print(f\"1. Full sequences:\")\n","print(f\"   - dl_sequences_full.parquet (all features)\")\n","print(f\"2. Train/test splits:\")\n","print(f\"   - dl_sequences_train.parquet\")\n","print(f\"   - dl_sequences_test.parquet\")\n","print(f\"3. Model-specific formats:\")\n","print(f\"   - dl_lstm_format.parquet (for LSTM models)\")\n","print(f\"   - dl_tft_format.parquet (for TFT models)\")\n","print(f\"4. Reference:\")\n","print(f\"   - dl_normalization_stats.json\")\n","\n","print(f\"\\nðŸŽ¯ UNIFIED TEST DATES:\")\n","print(f\"âœ“ All DL models predict the same {len(dl_test):,} test sequences\")\n","print(f\"âœ“ Same test period as baseline/ML models\")\n","\n","print(f\"\\nðŸ”’ DATA LEAKAGE PREVENTION:\")\n","print(f\"âœ“ Single target separated from input features\")\n","print(f\"âœ“ No 'current period' features in inputs\")\n","print(f\"âœ“ Only historical data (hist_) used for prediction\")\n","print(f\"âœ“ Static features constant over time\")\n","print(f\"âœ“ Prediction time features (pred_) for context only\")\n","\n","print(f\"\\nðŸ“Š SIMPLIFIED FEATURE STRUCTURE:\")\n","print(f\"âœ“ Historical sequence: {52 * 2:,} features (52 weeks Ã— 2 per week)\")\n","print(f\"âœ“ Static user features: 4 features\")\n","print(f\"âœ“ Sequence statistics: 3 features\")\n","print(f\"âœ“ Prediction time context: 2 features\")\n","print(f\"âœ“ Total input features: {52 * 2 + 4 + 3 + 2:,}\")\n","\n","print(f\"\\nâš¡ PERFORMANCE:\")\n","print(f\"âœ“ 20% sample for faster processing\")\n","print(f\"âœ“ Removed redundant features\")\n","print(f\"âœ“ Parquet format for fast I/O\")\n","print(f\"âœ“ Proper temporal constraints (no data leakage)\")\n","\n","print(f\"\\nðŸš€ Ready for model training!\")\n","print(f\"   - LSTM: Use extract_lstm_arrays() function\")\n","print(f\"   - TFT: Use dl_tft_format.parquet\")\n","print(f\"   - Custom: Use X_train, y_train arrays\")\n","\n","print(f\"\\nðŸ’¡ TRAINING EXAMPLES:\")\n","print(f\"   # LSTM: lstm_train_seq{lstm_train_seq.shape}, lstm_train_y{lstm_train_y.shape}\")\n","print(f\"   # Dense: X_train{X_train.shape}, y_train{y_train.shape}\")\n","print(f\"   # Target properly separated for supervised learning\")"]},{"cell_type":"code","source":["dl_sequences_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"id":"ssKsOFb0etjo","executionInfo":{"status":"error","timestamp":1748881885573,"user_tz":-180,"elapsed":9,"user":{"displayName":"V L","userId":"06129047709976680873"}},"outputId":"9e604874-e1cc-4431-e90b-ad37c0ebf1ae"},"execution_count":15,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'dl_sequences_train' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-7c517e006c69>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdl_sequences_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'dl_sequences_train' is not defined"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
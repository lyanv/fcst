{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing with proper target creation...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Starting preprocessing with proper target creation...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: local\n",
      "Base path: ..\n"
     ]
    }
   ],
   "source": [
    "def detect_environment():\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive/')\n",
    "        return 'colab', '/content/drive/MyDrive/fcst'\n",
    "    except ImportError:\n",
    "        return 'local', '..'\n",
    "\n",
    "environment, base_path = detect_environment()\n",
    "print(f\"Environment: {environment}\")\n",
    "print(f\"Base path: {base_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking data formats...\n",
      "  ✓ Parquet already exists: transactions_data.parquet\n",
      "  ✓ Parquet already exists: users_data.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nChecking data formats...\")\n",
    "\n",
    "def convert_to_parquet_if_needed(csv_path, parquet_path):\n",
    "    \"\"\"Convert CSV to parquet if parquet doesn't exist\"\"\"\n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"  ✓ Parquet already exists: {os.path.basename(parquet_path)}\")\n",
    "        return False\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "    \n",
    "    print(f\"  Converting {os.path.basename(csv_path)} to parquet...\")\n",
    "    start = datetime.now()\n",
    "    \n",
    "    # Read CSV with optimized dtypes\n",
    "    if 'transactions' in csv_path:\n",
    "        # Read in chunks for large transaction file\n",
    "        chunk_size = 1_000_000\n",
    "        chunks = []\n",
    "        for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size)):\n",
    "            chunk['amount'] = chunk['amount'].str.replace('$', '').astype('float32')\n",
    "            chunk['client_id'] = chunk['client_id'].astype('int32')\n",
    "            chunk['mcc'] = chunk['mcc'].astype('category')\n",
    "            chunks.append(chunk)\n",
    "            print(f\"    Processed {(i+1) * chunk_size:,} rows...\", end='\\r')\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "    else:\n",
    "        # Regular read for smaller files\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Optimize dtypes for users data\n",
    "        for col in ['per_capita_income', 'yearly_income', 'total_debt']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).str.replace('$', '').str.replace(',', '').astype('float32')\n",
    "    \n",
    "    # Save as parquet\n",
    "    df.to_parquet(parquet_path, compression='snappy')\n",
    "    \n",
    "    # Report compression\n",
    "    csv_size = os.path.getsize(csv_path) / (1024**2)\n",
    "    parquet_size = os.path.getsize(parquet_path) / (1024**2)\n",
    "    \n",
    "    print(f\"\\n  ✓ Converted: {csv_size:.1f} MB → {parquet_size:.1f} MB ({csv_size/parquet_size:.1f}x compression)\")\n",
    "    print(f\"  Time: {(datetime.now() - start).total_seconds():.1f} seconds\")\n",
    "    return True\n",
    "\n",
    "# Convert data files\n",
    "transactions_csv = f'{base_path}/data/transactions_data.csv'\n",
    "transactions_parquet = f'{base_path}/data/transactions_data.parquet'\n",
    "users_csv = f'{base_path}/data/users_data.csv'\n",
    "users_parquet = f'{base_path}/data/users_data.parquet'\n",
    "\n",
    "convert_to_parquet_if_needed(transactions_csv, transactions_parquet)\n",
    "convert_to_parquet_if_needed(users_csv, users_parquet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from parquet format...\n",
      "Data loaded in 2.1 seconds\n",
      "Transactions: 13,305,915\n",
      "Users: 2,000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading data from parquet format...\")\n",
    "start = datetime.now()\n",
    "\n",
    "df = pd.read_parquet(transactions_parquet)\n",
    "users_df = pd.read_parquet(users_parquet)\n",
    "\n",
    "with open(f'{base_path}/data/mcc_mapping.json', 'r') as f:\n",
    "    mcc_mapping = json.load(f)\n",
    "\n",
    "print(f\"Data loaded in {(datetime.now() - start).total_seconds():.1f} seconds\")\n",
    "print(f\"Transactions: {len(df):,}\")\n",
    "print(f\"Users: {len(users_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic preprocessing...\n",
      "Top 5 categories: ['food', 'transport', 'retail', 'services', 'specialty']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBasic preprocessing...\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['amount'] = df['amount'].abs()\n",
    "\n",
    "# Data types should already be optimized from parquet\n",
    "# Just ensure mcc is string for mapping\n",
    "df['mcc'] = df['mcc'].astype(str)\n",
    "\n",
    "# Clean user data\n",
    "for col in ['per_capita_income', 'yearly_income', 'total_debt']:\n",
    "    if col in users_df.columns:\n",
    "        users_df[col] = users_df[col].astype(str).str.replace('$', '').str.replace(',', '').astype(float)\n",
    "\n",
    "# Map MCC to categories\n",
    "mcc_to_category = {}\n",
    "for category, info in mcc_mapping['categories'].items():\n",
    "    for mcc_code in info['mcc_codes']:\n",
    "        mcc_to_category[str(mcc_code)] = category\n",
    "\n",
    "df['category'] = df['mcc'].map(mcc_to_category).fillna('other')\n",
    "\n",
    "# Get top 5 categories\n",
    "top_5_categories = df[df['category'] != 'other']['category'].value_counts().head(5).index.tolist()\n",
    "df = df[df['category'].isin(top_5_categories)]\n",
    "\n",
    "print(f\"Top 5 categories: {top_5_categories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating weekly aggregates...\n",
      "Final series: 6026\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating weekly aggregates...\")\n",
    "df['week'] = df['date'].dt.to_period('W')\n",
    "weekly_data = df.groupby(['client_id', 'category', 'week'])['amount'].sum().reset_index()\n",
    "\n",
    "# Filter series\n",
    "series_lengths = weekly_data.groupby(['client_id', 'category']).size()\n",
    "valid_series = series_lengths[series_lengths >= 104]\n",
    "\n",
    "def calculate_nonzero_ratio(group):\n",
    "    return (group['amount'] > 0).mean()\n",
    "\n",
    "nonzero_ratios = weekly_data.set_index(['client_id', 'category']).loc[valid_series.index].groupby(['client_id', 'category']).apply(calculate_nonzero_ratio)\n",
    "final_series = nonzero_ratios[nonzero_ratios >= 0.30]\n",
    "\n",
    "weekly_data = weekly_data.set_index(['client_id', 'category']).loc[final_series.index].reset_index()\n",
    "weekly_data['date'] = weekly_data['week'].dt.start_time\n",
    "weekly_data = weekly_data.sort_values(['client_id', 'category', 'date'])\n",
    "\n",
    "print(f\"Final series: {len(final_series)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging with user data...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMerging with user data...\")\n",
    "weekly_data = weekly_data.merge(users_df.rename(columns={'id': 'client_id'}), on='client_id', how='left')\n",
    "\n",
    "# Add basic transformations\n",
    "weekly_data['log_amount'] = np.log1p(weekly_data['amount'])\n",
    "weekly_data['sqrt_amount'] = np.sqrt(weekly_data['amount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating target (next week's amount)...\n",
      "Data with targets: 2,298,956 records\n",
      "Target columns: target (next week's amount), target_log, target_sqrt\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating target (next week's amount)...\")\n",
    "\n",
    "def create_target(group):\n",
    "    \"\"\"Create target as NEXT week's amount\"\"\"\n",
    "    group = group.sort_values('date')\n",
    "    # Target is the NEXT week's amount\n",
    "    group['target'] = group['amount'].shift(-1)\n",
    "    # Also create log and sqrt targets for different model types\n",
    "    group['target_log'] = group['log_amount'].shift(-1)\n",
    "    group['target_sqrt'] = group['sqrt_amount'].shift(-1)\n",
    "    return group\n",
    "\n",
    "weekly_data = weekly_data.groupby(['client_id', 'category']).apply(create_target).reset_index(drop=True)\n",
    "\n",
    "# Remove last observation per series (no target available)\n",
    "weekly_data = weekly_data.dropna(subset=['target'])\n",
    "\n",
    "print(f\"Data with targets: {len(weekly_data):,} records\")\n",
    "print(\"Target columns: target (next week's amount), target_log, target_sqrt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating train/test split...\n",
      "Train set: 1,836,701 records\n",
      "Test set: 462,255 records\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating train/test split...\")\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for (client_id, category), group in weekly_data.groupby(['client_id', 'category']):\n",
    "    n = len(group)\n",
    "    train_size = int(n * 0.8)\n",
    "    \n",
    "    train_data.append(group.iloc[:train_size])\n",
    "    test_data.append(group.iloc[train_size:])\n",
    "\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "test_df = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "print(f\"Train set: {len(train_df):,} records\")\n",
    "print(f\"Test set: {len(test_df):,} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying temporal integrity...\n",
      "✓ Target correctly represents next week's amount\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerifying temporal integrity...\")\n",
    "for (client_id, category), group in train_df.groupby(['client_id', 'category']):\n",
    "    group = group.sort_values('date')\n",
    "    # Check that target date is always after feature date\n",
    "    for i in range(len(group)-1):\n",
    "        current_date = group.iloc[i]['date']\n",
    "        next_date = group.iloc[i+1]['date']\n",
    "        target_amount = group.iloc[i]['target']\n",
    "        next_amount = group.iloc[i+1]['amount']\n",
    "        assert abs(target_amount - next_amount) < 0.01, f\"Target mismatch for {client_id}-{category}\"\n",
    "    break  # Just check one series\n",
    "\n",
    "print(\"✓ Target correctly represents next week's amount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving preprocessed data...\n",
      "\n",
      "=== PREPROCESSING SUMMARY ===\n",
      "Environment: local\n",
      "Data format: Parquet (5-10x faster than CSV)\n",
      "Train data: 1,836,701 records\n",
      "Test data: 462,255 records\n",
      "\n",
      "Target setup:\n",
      "- target: next week's amount (what we predict)\n",
      "- All features use data up to current week only\n",
      "- No data leakage: features at time t, target at time t+1\n",
      "\n",
      "Files saved (both CSV and Parquet):\n",
      "- ../data/preprocessed/train_with_target.csv (.parquet)\n",
      "- ../data/preprocessed/test_with_target.csv (.parquet)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving preprocessed data...\")\n",
    "output_dir = f'{base_path}/data/preprocessed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "train_df.to_csv(f'{output_dir}/train_with_target.csv', index=False)\n",
    "test_df.to_csv(f'{output_dir}/test_with_target.csv', index=False)\n",
    "\n",
    "# Also save as parquet for faster loading\n",
    "train_df.to_parquet(f'{output_dir}/train_with_target.parquet', index=False)\n",
    "test_df.to_parquet(f'{output_dir}/test_with_target.parquet', index=False)\n",
    "\n",
    "print(\"\\n=== PREPROCESSING SUMMARY ===\")\n",
    "print(f\"Environment: {environment}\")\n",
    "print(f\"Data format: Parquet (5-10x faster than CSV)\")\n",
    "print(f\"Train data: {len(train_df):,} records\")\n",
    "print(f\"Test data: {len(test_df):,} records\")\n",
    "print(\"\\nTarget setup:\")\n",
    "print(\"- target: next week's amount (what we predict)\")\n",
    "print(\"- All features use data up to current week only\")\n",
    "print(\"- No data leakage: features at time t, target at time t+1\")\n",
    "print(\"\\nFiles saved (both CSV and Parquet):\")\n",
    "print(f\"- {output_dir}/train_with_target.csv (.parquet)\")\n",
    "print(f\"- {output_dir}/test_with_target.csv (.parquet)\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
